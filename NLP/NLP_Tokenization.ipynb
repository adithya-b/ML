{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice=gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "sample_text=\"We will discuss briefly about the basic syntax,\\\n",
    " structure and design philosophies. \\\n",
    " There is a defined hierarchical syntax for Python code which you should remember \\\n",
    " when writing code! Python is a really powerful programming language!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentence Tokenizer\n",
    "* nltk.sent_tokenize is the default sentence tokenizer recommended by nltk\n",
    "* It is a pre-trained instance of the PunktSentenceTokenizer which works on various languages\n",
    "* Doesn't just use periods;Also considers other punctuations and capitalization of words to delimit sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_st=nltk.sent_tokenize\n",
    "alice_sentences=default_st(text=alice)\n",
    "sample_sentences=default_st(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total alice sentences: 1625 \n",
      " [\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\", \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"] \n",
      "\n",
      "\n",
      "total sample sentences: 3 \n",
      " ['We will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember  when writing code!']\n"
     ]
    }
   ],
   "source": [
    "print(\"total alice sentences:\",len(alice_sentences),\"\\n\",alice_sentences[0:2],\"\\n\\n\")\n",
    "print(\"total sample sentences:\",len(sample_sentences),\"\\n\",sample_sentences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing German Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text=europarl_raw.german.raw(fileids=\"ep-00-01-17.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157171"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_sentences_def=default_st(text=german_text,language=\"german\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Of Models for above usage**\n",
    "* The below code shows that indeed there is indeed a trained model for above task under the punkt in tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer=nltk.data.load(resource_url=\"/Users/adithyabandi/nltk_data/tokenizers/punkt/german.pickle\")\n",
    "german_sentences=german_tokenizer.tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences==german_sentences_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total german sentences: 938 \n",
      " [' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .', 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .']\n"
     ]
    }
   ],
   "source": [
    "print(\"total german sentences:\",len(german_sentences_def),\"\\n\",german_sentences_def[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization Using PunktTokenizer Class**\n",
    "* Involves creation of above class instance and calling tokenize function using the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember  when writing code!', 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "punkt_st=nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences=punkt_st.tokenize(sample_text)\n",
    "print(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Tokenization using Regular Expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
       " ' There is a defined hierarchical syntax for Python code which you should remember  when writing code!',\n",
       " 'Python is a really powerful programming language!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st=nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "sample_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "* To be done after the sentence tokenization to get the words as tokens\n",
    "* The splitting are done based on periods,commas,single quotes.\n",
    "* Most punctuation characters are split and separated into independent words.\n",
    "* Split words with contractions.eg: do and n't for don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt=nltk.word_tokenize\n",
    "words=default_wt(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TreebankWordTokenizer**\n",
    "* The above instance is a function of TreebankWordTokenizer class and can be used directly instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank=nltk.tokenize.TreebankWordTokenizer()\n",
    "words=treebank.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Tokenization using Regular Expressions**\n",
    "* Pattern parameter dictates the pattern to be tokenized.\n",
    "* Gaps parameter tells whether it's the gaps between the patterns to be tokenized or the tokens themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'wasn',\n",
       " 't',\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_PATTERN=r'\\w+'\n",
    "regex_wt=nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,gaps=False)\n",
    "words=regex_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " \"wasn't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " \"couldn't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAP_PATTERN=r\"\\s+\"\n",
    "regex_wt=nltk.RegexpTokenizer(pattern=GAP_PATTERN,gaps=True)\n",
    "words=regex_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "#To get the start and end indices of the token along with the tokens.\n",
    "word_indices=list(regex_wt.span_tokenize(sentence))\n",
    "word_indices\n",
    "print([sentence[start:end] for start,end in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Word Tokenizers**\n",
    "* WordPunktTokenizer: Uses the pattern: r'\\w+|[^w\\s]+'\n",
    "* WhitespaceTokenizer: Splits based on whitespaces like tabs,newlines,spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_punkt=nltk.tokenize.WordPunctTokenizer()\n",
    "words=word_punkt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " \"wasn't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " \"couldn't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_wt=nltk.tokenize.WhitespaceTokenizer()\n",
    "words=whitespace_wt.tokenize(sentence)\n",
    "words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
