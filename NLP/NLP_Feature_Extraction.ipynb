{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bag Of Words **\n",
    "* Scikitlearn CountVectorizer acts as the Bag of words.\n",
    "* The ngram_range specifies the ngrams to be considered for vectorizing.\n",
    "* (1,1) indicates unigram which are individual words.\n",
    "* (1,2) indicates unigrams as well as bigrams and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bow_extractor(corpus,ngram_range=(1,1)):\n",
    "    vectorizer=CountVectorizer(min_df=1,ngram_range=ngram_range)\n",
    "    features=vectorizer.fit_transform(corpus)\n",
    "    return vectorizer,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "'the sky is blue',\n",
    "'sky is blue and sky is beautiful',\n",
    "'the beautiful sky is so blue',\n",
    "'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 1, 0, 2, 0, 2, 0, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer,bow_features=bow_extractor(CORPUS)\n",
    "features=bow_features.todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer.transform(new_doc).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 2,\n",
       "         2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer,bow_features=bow_extractor(CORPUS,ngram_range=(1,3))\n",
    "features=bow_features.todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidfextractor(corpus,ngram_range=(1,1)):\n",
    "    vectorizer=TfidfVectorizer(min_df=1,norm=\"l2\",smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n",
    "    features=vectorizer.fit_transform(corpus)\n",
    "    return vectorizer,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39921021 0.         0.48829139 0.\n",
      "  0.48829139 0.         0.60313701]\n",
      " [0.44051607 0.34730793 0.22987956 0.         0.5623514  0.\n",
      "  0.5623514  0.         0.        ]\n",
      " [0.         0.43202578 0.28595344 0.         0.3497621  0.\n",
      "  0.3497621  0.54796992 0.43202578]\n",
      " [0.         0.         0.34618161 0.66338461 0.         0.66338461\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.63295194, 0.        , 0.        ,\n",
       "         0.        , 0.77419109, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvectorizer,features=tfidfextractor(CORPUS)\n",
    "print(features.todense())\n",
    "tfidfvectorizer.transform(new_doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word2Vec**\n",
    "* Word2Vec model is released by Google which is based on bag of words and skip gram based architectures.\n",
    "* This gives the words in a n dimensional space where the words with similar meaning are placed closer to each other.\n",
    "* Below is gensim implementation of Word2Vec.The parameters are as follows:\n",
    "* size:the total number of features in feature space\n",
    "* window:the length of window of words to be considered in the algorithm\n",
    "* min_count:minimum count of words across documents to be considered.To ignore sparse words of no interest.\n",
    "* sample: To downsample occurence of most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_CORPUS=[nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc=[nltk.word_tokenize(sentence) for sentence in new_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=gensim.models.Word2Vec(TOKENIZED_CORPUS,size=10,window=10,min_count=2,sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00893308  0.02171977  0.04001991  0.02354194 -0.01633224  0.02735614\n",
      " -0.00345451  0.00202995  0.03671695 -0.04223331]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.0188542e-03,  2.6809189e-02,  3.8394030e-02, -2.6277043e-02,\n",
       "        3.2703634e-02,  2.0800639e-02,  3.6418807e-02, -3.9342128e-02,\n",
       "        2.8878924e-02, -6.8178852e-05], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to use the model to extract features for every single word.\n",
    "print(model[\"sky\"])\n",
    "print(model[\"blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
