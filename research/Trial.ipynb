{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"NegInstancesWithDegrees.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=[x.strip() for x in f]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for line in lines:\n",
    "degree_values={}    \n",
    "for line in lines:\n",
    "    perinstance_degrees=line.split(\":\")\n",
    "    key=perinstance_degrees[0]\n",
    "    perinstance_degrees_values=perinstance_degrees[1].split(\",\")\n",
    "    dic={}\n",
    "    for perinstance_degrees_value in perinstance_degrees_values:\n",
    "        temp=perinstance_degrees_value.split(\"-\")\n",
    "        dic[temp[0]]=temp[1]\n",
    "    degree_values[key]=dic\n",
    "#print(degree_values['cabbage/cabbage_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_features(file_name):\n",
    "    f=open(file_name)\n",
    "    lines=[x.split() for x in f]\n",
    "    result=[]\n",
    "    for line in lines:\n",
    "        temp_array=line[0].split(\",\")\n",
    "        temp_array=[int(temp) for temp in temp_array]\n",
    "        result.extend(temp_array)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_features_considered=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    36\n",
       "12    24\n",
       "9      7\n",
       "18     5\n",
       "6      2\n",
       "21     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rootdir = '/Users/adithyabandi/Downloads/Summer18/Negatives/nTz'\n",
    "feature_set={}\n",
    "ming=1234\n",
    "g=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        file_name=os.path.join(subdir, file)\n",
    "        if \"_rgb.log\" in file_name:\n",
    "            temp_array=file_name.split(\"/\")\n",
    "            temp_array=temp_array[len(temp_array)-1].split(\"_rgb.log\")\n",
    "            features=get_color_features(file_name)\n",
    "            temp=temp_array[0].split(\"_\")\n",
    "            temp_features=np.zeros(no_of_features_considered)\n",
    "            g.append(len(features))\n",
    "            temp_len=len(features)\n",
    "            if temp_len>no_of_features_considered:\n",
    "                #print(temp_len,file_name)\n",
    "                temp_len=no_of_features_considered\n",
    "            temp_features[:temp_len]=features[:temp_len]\n",
    "            feature_set[temp[0]+\"/\"+temp_array[0]]=list(temp_features)\n",
    "pd.Series(g).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def get_perkey_pos_instances(filename):\n",
    "    f=open(filename)\n",
    "    lines=[x.strip() for x in f]\n",
    "    f.close()\n",
    "    token_instances={}\n",
    "    for line in lines:\n",
    "        temp=line.split(\":\")\n",
    "        token_instances[temp[0]]=temp[1]\n",
    "    #print(token_instances[\"red\"])\n",
    "    maxv=-1\n",
    "    minv=10\n",
    "    perkey_pos_instances={}\n",
    "    for key,_ in token_instances.items():\n",
    "        temp_array=token_instances[key].split(\",\")\n",
    "        values_arr=[]\n",
    "        for i in range(len(temp_array)):\n",
    "            temp2=temp_array[i].split(\"(\")\n",
    "            if len(temp2)<2:\n",
    "                continue\n",
    "            count=int(temp2[1].split(\")\")[0])\n",
    "            #print(len(temp_array),count)\n",
    "            tf=math.log(1+count)\n",
    "            idf=math.log(72/len(temp_array))\n",
    "            tf_idf=tf*idf\n",
    "            if tf_idf>1:\n",
    "                values_arr.append(temp2[0])\n",
    "        perkey_pos_instances[key]=values_arr\n",
    "    return perkey_pos_instances,maxv,minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 10 ['cabbage/cabbage_3', 'eggplant/eggplant_2', 'plum/plum_2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1,\n",
       " 10,\n",
       " ['cabbage/cabbage_1',\n",
       "  'cabbage/cabbage_2',\n",
       "  'cabbage/cabbage_4',\n",
       "  'carrot/carrot_2',\n",
       "  'eggplant/eggplant_1',\n",
       "  'eggplant/eggplant_3',\n",
       "  'eggplant/eggplant_4',\n",
       "  'plum/plum_1',\n",
       "  'plum/plum_3',\n",
       "  'plum/plum_4',\n",
       "  'potato/potato_1'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perkey_pos_instances,trmax,trmin=get_perkey_pos_instances(\"traintokens.txt\")\n",
    "perkeytest_pos_instances,temax,temin=get_perkey_pos_instances(\"testtokens.txt\")\n",
    "print(temax,temin,perkeytest_pos_instances[\"purpl\"])\n",
    "trmax,trmin,perkey_pos_instances[\"purpl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcd(a,b):\n",
    "    \"\"\"Compute the greatest common divisor of a and b\"\"\"\n",
    "    while b > 0:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "    \n",
    "def lcm(a, b):\n",
    "    \"\"\"Compute the lowest common multiple of a and b\"\"\"\n",
    "    return a * b / gcd(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Scoring and Validating Model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val_results(pos_inputs,neg_inputs,pos_labels,neg_labels):\n",
    "    pos_inputs=np.array(pos_inputs)\n",
    "    neg_inputs=np.array(neg_inputs)\n",
    "    \n",
    "    pos_labels=pos_labels.reshape(len(pos_labels),1)\n",
    "    neg_labels=neg_labels.reshape(len(neg_labels),1)\n",
    "    \n",
    "    pos_full=np.concatenate((pos_inputs,pos_labels),axis=1)\n",
    "    neg_full=np.concatenate((neg_inputs,neg_labels),axis=1)\n",
    "    \n",
    "    g=lcm(len(pos_labels),len(neg_labels))\n",
    "    g=int(g)\n",
    "          \n",
    "    pos_full_upsampled=resample(pos_full,replace=True,n_samples=g,random_state=42)\n",
    "    \n",
    "    neg_full_upsampled=resample(neg_full,replace=True,n_samples=g,random_state=42)\n",
    "    \n",
    "    all_=np.concatenate((pos_full_upsampled,neg_full_upsampled))\n",
    "    inputs=all_[:,:no_of_features_considered]\n",
    "    labels=all_[:,-1:]\n",
    "    \n",
    "    rf_clf=RandomForestClassifier(random_state=42)\n",
    "    gb_clf=GaussianNB()\n",
    "    sgd_clf=SGDClassifier(random_state=42)\n",
    "    rf_clf=RandomForestClassifier(random_state=42)\n",
    "    gb_clf=GradientBoostingClassifier(random_state=42)\n",
    "    knn_clf=KNeighborsClassifier()\n",
    "    sv_clf=SVC(random_state=42)\n",
    "    lsv_clf=LinearSVC(random_state=42)#LinearSVC(random_state=42,penalty=\"l1\",dual=False)\n",
    "    \n",
    "    lsv_clf.fit(inputs,labels)\n",
    "    pred=lsv_clf.predict(inputs)#test_inputs)\n",
    "    acc_score=accuracy_score(labels,pred)*100\n",
    "    prec_score=precision_score(labels,pred)*100\n",
    "    rec_score=recall_score(labels,pred)*100\n",
    "    f1=f1_score(labels,pred)*100\n",
    "\n",
    "\n",
    "    #cross_val_results=cross_val_score(lsv_clf,inputs,labels,cv=5,scoring=\"accuracy\")\n",
    "    #print(cross_val_results)\n",
    "    #print(np.mean(cross_val_results)*100)\n",
    "    #print(np.std(cross_val_results)*100)\n",
    "    return prec_score,rec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,f1_score,roc_auc_score\n",
    "\n",
    "#Scoring and Validating Model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def test_results(pos_inputs,neg_inputs,pos_labels,neg_labels,test_inputs,test_labels):\n",
    "    pos_inputs=np.array(pos_inputs)\n",
    "    neg_inputs=np.array(neg_inputs)\n",
    "    \n",
    "    test_inputs=np.array(test_inputs)\n",
    "    \n",
    "    pos_labels=pos_labels.reshape(len(pos_labels),1)\n",
    "    neg_labels=neg_labels.reshape(len(neg_labels),1)\n",
    "    \n",
    "    test_labels=test_labels.reshape(len(test_labels),1)\n",
    "    \n",
    "    pos_full=np.concatenate((pos_inputs,pos_labels),axis=1)\n",
    "    neg_full=np.concatenate((neg_inputs,neg_labels),axis=1)\n",
    "    \n",
    "    g=lcm(len(pos_labels),len(neg_labels))\n",
    "    g=int(g)\n",
    "          \n",
    "    pos_full_upsampled=resample(pos_full,replace=True,n_samples=g,random_state=42)\n",
    "    \n",
    "    neg_full_upsampled=resample(neg_full,replace=True,n_samples=g,random_state=42)\n",
    "    \n",
    "    all_=np.concatenate((pos_full_upsampled,neg_full_upsampled))\n",
    "    inputs=all_[:,:no_of_features_considered]\n",
    "    labels=all_[:,-1:]\n",
    "    \n",
    "    split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "    \n",
    "    #all_=pd.DataFrame(all_)\n",
    "    #for train_index,test_index in split.split(all_,all_[6]):\n",
    "     #   train=all_.loc[train_index]\n",
    "      #  test=all_.loc[test_index]\n",
    "    \n",
    "    rf_clf=RandomForestClassifier(random_state=42)\n",
    "    gb_clf=GaussianNB()\n",
    "    sgd_clf=SGDClassifier(random_state=42)\n",
    "    rf_clf=RandomForestClassifier(random_state=42)\n",
    "    gb_clf=GradientBoostingClassifier(random_state=42)\n",
    "    knn_clf=KNeighborsClassifier()\n",
    "    sv_clf=SVC(kernel=\"rbf\",random_state=42)\n",
    "    lsv_clf=LinearSVC(random_state=42)#LinearSVC(random_state=42,penalty=\"l1\",dual=False)\n",
    "    \n",
    "    #lsv_clf.fit(train[[0,1,2,3,4,5]],train[[6]])\n",
    "    \n",
    "    #pred=lsv_clf.predict(test[[0,1,2,3,4,5]])\n",
    "    #acc_score=accuracy_score(test[[6]],pred)*100\n",
    "    #prec_score=precision_score(test[[6]],pred)*100\n",
    "    #rec_score=recall_score(test[[6]],pred)*100\n",
    "    #f1=f1_score(test[[6]],pred)*100\n",
    "    \n",
    "    lsv_clf.fit(inputs,labels)\n",
    "    pred=lsv_clf.predict(inputs)#test_inputs)\n",
    "    acc_score=accuracy_score(labels,pred)*100\n",
    "    prec_score=precision_score(labels,pred)*100\n",
    "    rec_score=recall_score(labels,pred)*100\n",
    "    f1=f1_score(labels,pred)*100\n",
    "    \n",
    "    print(\"train scores:\",acc_score,prec_score,rec_score,f1)\n",
    "    \n",
    "    lsv_clf.fit(inputs,labels)\n",
    "    pred=lsv_clf.predict(test_inputs)\n",
    "    print(pred)\n",
    "    acc_score=accuracy_score(test_labels,pred)*100\n",
    "    prec_score=precision_score(test_labels,pred)*100\n",
    "    rec_score=recall_score(test_labels,pred)*100\n",
    "    f1=f1_score(test_labels,pred)*100\n",
    "    \n",
    "    \n",
    "    return acc_score,prec_score,rec_score,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_neg_instances_per_token(token,accuracy_threshold):\n",
    "    pos_inputs=[]\n",
    "    neg_inputs=[]\n",
    "    neg_instances=[]\n",
    "    pos_instances=perkey_pos_instances[token]\n",
    "    test_instances=perkeytest_pos_instances[token]\n",
    "    for instance in pos_instances:\n",
    "        pos_inputs.append(feature_set[instance])\n",
    "    for ins in pos_instances:\n",
    "        #pos_inputs.append(feature_set[instance])\n",
    "        temp=degree_values[ins]\n",
    "        count=0\n",
    "        for instance in reversed(list(temp.keys())):\n",
    "            if (instance not in pos_instances) & (instance not in neg_instances) & (instance not in test_instances):\n",
    "                neg_inputs.append(feature_set[instance])\n",
    "                pos_labels=np.ones(len(pos_inputs))\n",
    "                neg_labels=np.zeros(len(neg_inputs))\n",
    "                g,f=cross_val_results(pos_inputs,neg_inputs,pos_labels,neg_labels)\n",
    "                #print(len(neg_inputs),g,f)\n",
    "                if g<accuracy_threshold or f<accuracy_threshold:\n",
    "                    neg_inputs.remove(feature_set[instance])\n",
    "                    #print(ins,instance,temp[instance])\n",
    "                    count+=1\n",
    "                    g,f=cross_val_results(pos_inputs,neg_inputs,pos_labels,np.zeros(len(neg_inputs)))\n",
    "                    if count>3:\n",
    "                        #print(\"final results\",g)\n",
    "                        break\n",
    "                    continue\n",
    "                else:\n",
    "                    neg_instances.append(instance)\n",
    "    return pos_instances,neg_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_samples(pos_instances,neg_instances,test_instances):\n",
    "    #print(len(pos_instances))\n",
    "    #print(len(neg_instances))\n",
    "    neg_inputs=[]\n",
    "    for instance in neg_instances:\n",
    "        neg_inputs.append(feature_set[instance])\n",
    "    pos_inputs=[]\n",
    "    for instance in pos_instances:\n",
    "        pos_inputs.append(feature_set[instance])\n",
    "    test_inputs=[]\n",
    "    for instance in test_instances:\n",
    "        test_inputs.append(feature_set[instance])\n",
    "    pos_labels=np.ones(len(pos_inputs))\n",
    "    neg_labels=np.zeros(len(neg_inputs))\n",
    "    test_labels=np.ones(len(test_inputs))\n",
    "    return test_results(pos_inputs,neg_inputs,pos_labels,neg_labels,test_inputs,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[\"whit\",\"blu\",\"yellow\",\"red\",\"purpl\",\"orang\",\"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: whit\n",
      "train scores: 100.0 100.0 100.0 100.0\n",
      "[1. 1. 1.]\n",
      "whit 100.0 100.0 100.0 100.0 11 24 \n",
      "\n",
      "['arch/arch_1', 'banana/banana_2', 'corn/corn_1', 'corn/corn_2', 'corn/corn_3', 'eggplant/eggplant_1', 'eggplant/eggplant_3', 'lemon/lemon_1', 'potato/potato_4', 'semicylinder/semicylinder_2', 'semicylinder/semicylinder_3'] \n",
      "\n",
      "['lime/lime_2', 'cabbage/cabbage_2', 'lime/lime_3', 'cabbage/cabbage_1', 'cabbage/cabbage_4', 'lime/lime_1', 'arch/arch_2', 'semicylinder/semicylinder_1', 'semicylinder/semicylinder_4', 'cylinder/cylinder_3', 'arch/arch_3', 'cube/cube_3', 'cube/cube_2', 'cylinder/cylinder_4', 'cuboid/cuboid_2', 'cube/cube_1', 'triangle/triangle_2', 'cuboid/cuboid_4', 'triangle/triangle_4', 'banana/banana_4', 'cucumber/cucumber_3', 'cucumber/cucumber_1', 'cucumber/cucumber_2', 'cucumber/cucumber_4'] \n",
      "\n",
      "token: blu\n",
      "train scores: 100.0 100.0 100.0 100.0\n",
      "[1. 1.]\n",
      "blu 100.0 100.0 100.0 100.0 5 65 \n",
      "\n",
      "['arch/arch_2', 'arch/arch_3', 'cuboid/cuboid_4', 'cylinder/cylinder_3', 'triangle/triangle_4'] \n",
      "\n",
      "['banana/banana_2', 'tomato/tomato_4', 'banana/banana_3', 'orange/orange_2', 'lemon/lemon_4', 'tomato/tomato_3', 'orange/orange_4', 'banana/banana_4', 'lemon/lemon_2', 'tomato/tomato_1', 'orange/orange_1', 'tomato/tomato_2', 'plum/plum_4', 'lime/lime_2', 'lemon/lemon_3', 'banana/banana_1', 'potato/potato_1', 'lemon/lemon_1', 'lime/lime_3', 'orange/orange_3', 'plum/plum_3', 'plum/plum_2', 'potato/potato_2', 'eggplant/eggplant_1', 'plum/plum_1', 'cylinder/cylinder_1', 'lime/lime_4', 'cylinder/cylinder_2', 'lime/lime_1', 'cube/cube_1', 'cabbage/cabbage_4', 'cabbage/cabbage_3', 'potato/potato_3', 'cabbage/cabbage_2', 'eggplant/eggplant_2', 'cabbage/cabbage_1', 'cube/cube_2', 'cube/cube_4', 'cuboid/cuboid_3', 'potato/potato_4', 'eggplant/eggplant_4', 'cucumber/cucumber_1', 'cylinder/cylinder_4', 'cucumber/cucumber_2', 'carrot/carrot_3', 'cucumber/cucumber_3', 'eggplant/eggplant_3', 'cucumber/cucumber_4', 'cuboid/cuboid_1', 'cuboid/cuboid_2', 'carrot/carrot_1', 'carrot/carrot_4', 'carrot/carrot_2', 'triangle/triangle_2', 'corn/corn_1', 'triangle/triangle_1', 'triangle/triangle_3', 'corn/corn_3', 'semicylinder/semicylinder_3', 'corn/corn_4', 'semicylinder/semicylinder_4', 'corn/corn_2', 'semicylinder/semicylinder_2', 'arch/arch_1', 'arch/arch_4'] \n",
      "\n",
      "token: yellow\n",
      "train scores: 98.15789473684211 100.0 96.3157894736842 98.12332439678283\n",
      "[0. 1. 1. 1. 1.]\n",
      "yellow 80.0 100.0 80.0 88.8888888888889 20 19 \n",
      "\n",
      "['arch/arch_1', 'banana/banana_1', 'banana/banana_2', 'banana/banana_3', 'corn/corn_1', 'corn/corn_2', 'corn/corn_3', 'cube/cube_1', 'cuboid/cuboid_1', 'cylinder/cylinder_1', 'lemon/lemon_1', 'lemon/lemon_2', 'lemon/lemon_3', 'orange/orange_1', 'orange/orange_2', 'orange/orange_3', 'potato/potato_4', 'semicylinder/semicylinder_3', 'tomato/tomato_4', 'triangle/triangle_2'] \n",
      "\n",
      "['lime/lime_2', 'cabbage/cabbage_3', 'eggplant/eggplant_1', 'cabbage/cabbage_2', 'lime/lime_3', 'cabbage/cabbage_1', 'lime/lime_1', 'plum/plum_3', 'semicylinder/semicylinder_1', 'cylinder/cylinder_4', 'semicylinder/semicylinder_4', 'cuboid/cuboid_2', 'cabbage/cabbage_4', 'cuboid/cuboid_4', 'cube/cube_3', 'arch/arch_3', 'cube/cube_2', 'cucumber/cucumber_3', 'cucumber/cucumber_1'] \n",
      "\n",
      "token: red\n",
      "train scores: 100.0 100.0 100.0 100.0\n",
      "[1. 1. 1. 1. 0. 1. 1.]\n",
      "red 85.71428571428571 100.0 85.71428571428571 92.3076923076923 18 45 \n",
      "\n",
      "['cabbage/cabbage_1', 'cabbage/cabbage_2', 'cabbage/cabbage_4', 'carrot/carrot_2', 'carrot/carrot_4', 'cube/cube_4', 'cylinder/cylinder_2', 'orange/orange_1', 'plum/plum_1', 'plum/plum_3', 'plum/plum_4', 'potato/potato_1', 'potato/potato_2', 'semicylinder/semicylinder_2', 'tomato/tomato_1', 'tomato/tomato_3', 'tomato/tomato_4', 'triangle/triangle_1'] \n",
      "\n",
      "['cuboid/cuboid_1', 'cube/cube_1', 'cuboid/cuboid_2', 'arch/arch_1', 'cylinder/cylinder_1', 'triangle/triangle_2', 'potato/potato_4', 'semicylinder/semicylinder_3', 'cube/cube_2', 'banana/banana_1', 'triangle/triangle_3', 'arch/arch_3', 'potato/potato_3', 'lemon/lemon_4', 'orange/orange_2', 'cucumber/cucumber_2', 'cuboid/cuboid_4', 'lemon/lemon_1', 'arch/arch_2', 'orange/orange_3', 'cube/cube_3', 'cucumber/cucumber_1', 'cylinder/cylinder_4', 'semicylinder/semicylinder_4', 'cucumber/cucumber_3', 'semicylinder/semicylinder_1', 'lemon/lemon_2', 'banana/banana_3', 'triangle/triangle_4', 'banana/banana_2', 'lime/lime_3', 'lime/lime_4', 'lemon/lemon_3', 'eggplant/eggplant_4', 'eggplant/eggplant_3', 'banana/banana_4', 'eggplant/eggplant_1', 'corn/corn_1', 'lime/lime_2', 'eggplant/eggplant_2', 'corn/corn_4', 'corn/corn_3', 'corn/corn_2', 'cucumber/cucumber_4', 'cylinder/cylinder_3'] \n",
      "\n",
      "token: purpl\n",
      "train scores: 97.64890282131663 95.50898203592814 100.0 97.70290964777948\n",
      "[1. 1. 1.]\n",
      "purpl 100.0 100.0 100.0 100.0 11 29 \n",
      "\n",
      "['cabbage/cabbage_1', 'cabbage/cabbage_2', 'cabbage/cabbage_4', 'carrot/carrot_2', 'eggplant/eggplant_1', 'eggplant/eggplant_3', 'eggplant/eggplant_4', 'plum/plum_1', 'plum/plum_3', 'plum/plum_4', 'potato/potato_1'] \n",
      "\n",
      "['cuboid/cuboid_1', 'cube/cube_1', 'cuboid/cuboid_2', 'arch/arch_1', 'cylinder/cylinder_1', 'triangle/triangle_2', 'potato/potato_4', 'semicylinder/semicylinder_3', 'cube/cube_2', 'banana/banana_1', 'triangle/triangle_3', 'arch/arch_3', 'potato/potato_3', 'lemon/lemon_4', 'orange/orange_2', 'cucumber/cucumber_2', 'lemon/lemon_1', 'arch/arch_2', 'orange/orange_3', 'orange/orange_1', 'banana/banana_2', 'lemon/lemon_3', 'banana/banana_3', 'semicylinder/semicylinder_4', 'cucumber/cucumber_4', 'cucumber/cucumber_1', 'cylinder/cylinder_4', 'corn/corn_1', 'semicylinder/semicylinder_2'] \n",
      "\n",
      "token: orang\n",
      "train scores: 100.0 100.0 100.0 100.0\n",
      "[1. 1. 1. 1.]\n",
      "orang 100.0 100.0 100.0 100.0 12 2 \n",
      "\n",
      "['carrot/carrot_2', 'carrot/carrot_3', 'carrot/carrot_4', 'lemon/lemon_1', 'lemon/lemon_2', 'lemon/lemon_3', 'orange/orange_1', 'orange/orange_2', 'orange/orange_3', 'potato/potato_1', 'semicylinder/semicylinder_3', 'tomato/tomato_4'] \n",
      "\n",
      "['semicylinder/semicylinder_4', 'triangle/triangle_2'] \n",
      "\n",
      "token: green\n",
      "train scores: 98.21428571428571 100.0 96.42857142857143 98.18181818181819\n",
      "[0. 1. 1. 1. 1. 1. 1.]\n",
      "green 85.71428571428571 100.0 85.71428571428571 92.3076923076923 14 4 \n",
      "\n",
      "['arch/arch_3', 'banana/banana_2', 'banana/banana_3', 'cube/cube_2', 'cuboid/cuboid_2', 'cucumber/cucumber_1', 'cucumber/cucumber_2', 'cucumber/cucumber_4', 'eggplant/eggplant_1', 'lime/lime_2', 'lime/lime_3', 'lime/lime_4', 'semicylinder/semicylinder_4', 'tomato/tomato_4'] \n",
      "\n",
      "['orange/orange_2', 'tomato/tomato_1', 'arch/arch_2', 'carrot/carrot_3'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(\"token:\",token)\n",
    "    #print(\"positive_instance cutoff_instance cutoff_degrees\")\n",
    "    pos_instances,neg_instances=get_pos_neg_instances_per_token(token,95)\n",
    "    test_score,pre_score,rec_score,f1=evaluate_samples(pos_instances,neg_instances,perkeytest_pos_instances[token])\n",
    "    print(token,test_score,pre_score,rec_score,f1,len(pos_instances),len(neg_instances),\"\\n\")\n",
    "    print(pos_instances,\"\\n\")\n",
    "    print(neg_instances,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/adithyabandi/Desktop/whit_token.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e81256a78906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtemp_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/adithyabandi/Desktop/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_token.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/adithyabandi/Desktop/whit_token.txt' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stats=None\n",
    "for token in tokens:\n",
    "    temp_df=pd.read_csv(\"/Users/adithyabandi/Desktop/\"+token+\"_token.txt\",sep=\" \")\n",
    "    temp_df[\"token\"]=token\n",
    "    if stats is not None:\n",
    "        stats=stats.append(temp_df)\n",
    "    else:\n",
    "        stats=temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=np.array([[1,2,3,4,None]])\n",
    "h=np.array([0,1,0,1,0])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "rf_clf=RandomForestClassifier(random_state=42)\n",
    "gb_clf=GaussianNB()\n",
    "sgd_clf=SGDClassifier(random_state=42)\n",
    "rf_clf=RandomForestClassifier(random_state=42)\n",
    "gb_clf=GradientBoostingClassifier(random_state=42)\n",
    "knn_clf=KNeighborsClassifier()\n",
    "sv_clf=SVC(kernel=\"rbf\",random_state=42)\n",
    "lsv_clf=LinearSVC(random_state=42)\n",
    "    \n",
    "\n",
    "l=LogisticRegression()\n",
    "\n",
    "lsv_clf.fit(g,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
