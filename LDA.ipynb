{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#remove=(\"headers\",\"footers\",\"quotes\")\n",
    "#texts=fetch_20newsgroups(subset=\"train\",remove=remove).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=['turkey day holiday',\n",
    "          'cake holiday',\n",
    "          'turkey race thanksgiving holiday',\n",
    "          'snail race turtle',\n",
    "          'time travel space race',\n",
    "          'movie thanksgiving',\n",
    "          'movie air space museum cool movie',\n",
    "          'aspiring movie star']\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure\"\n",
    "#doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3,  doc5]\n",
    "\n",
    "#docs=docs+doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_struct=[]\n",
    "for i in range(len(docs)):\n",
    "    docs_struct.append(docs[i].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['turkey', 'day', 'holiday'],\n",
       " ['cake', 'holiday'],\n",
       " ['turkey', 'race', 'thanksgiving', 'holiday'],\n",
       " ['snail', 'race', 'turtle'],\n",
       " ['time', 'travel', 'space', 'race'],\n",
       " ['movie', 'thanksgiving'],\n",
       " ['movie', 'air', 'space', 'museum', 'cool', 'movie'],\n",
       " ['aspiring', 'movie', 'star']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=np.array([])\n",
    "for i in range(len(docs_struct)):\n",
    "    g=np.append(g,docs_struct[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['air', 'aspiring', 'cake', 'cool', 'day', 'holiday', 'movie',\n",
       "       'museum', 'race', 'snail', 'space', 'star', 'thanksgiving', 'time',\n",
       "       'travel', 'turkey', 'turtle'], dtype='<U32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=np.unique(g)\n",
    "print(vocab.shape)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_count=np.zeros([n_topics,len(vocab)])\n",
    "topic_alloc=[]\n",
    "docs_pertopic_count=[]\n",
    "for i in range(len(docs_struct)):\n",
    "    topic_alloc.append([])\n",
    "    docs_pertopic_count.append(np.zeros(n_topics))\n",
    "    for j in range(len(docs_struct[i])):\n",
    "        for k in range(len(vocab)):\n",
    "            if(vocab[k]==docs_struct[i][j]):\n",
    "                docs_struct[i][j]=(k+1)\n",
    "                \n",
    "                topic_alloc[i].append(np.random.randint(n_topics))\n",
    "                \n",
    "                topic_index=topic_alloc[i][j]\n",
    "                word_index=docs_struct[i][j]\n",
    "                word_topic_count[topic_index][word_index-1]+=1\n",
    "                \n",
    "                docs_pertopic_count[i][topic_index]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 5, 6],\n",
       " [3, 6],\n",
       " [16, 9, 13, 6],\n",
       " [10, 9, 17],\n",
       " [14, 15, 11, 9],\n",
       " [7, 13],\n",
       " [7, 1, 11, 8, 4, 7],\n",
       " [2, 7, 12]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0. 1. 0. 4. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 2. 0. 1. 1. 1. 2. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "word_topic_count=np.array(word_topic_count)\n",
    "word_topic_count\n",
    "print(word_topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0, 1],\n",
       " [0, 2],\n",
       " [2, 0, 0, 1],\n",
       " [1, 1, 0],\n",
       " [2, 2, 1, 2],\n",
       " [0, 2],\n",
       " [0, 2, 1, 1, 2, 0],\n",
       " [0, 0, 2]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [2., 1., 1.],\n",
       "       [1., 2., 0.],\n",
       "       [0., 1., 3.],\n",
       "       [1., 0., 1.],\n",
       "       [2., 2., 2.],\n",
       "       [2., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pertopic_count=np.array(docs_pertopic_count)\n",
    "docs_pertopic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.2 0.4 0.4] 1\n",
      "0 [0.25 0.25 0.5 ] 2\n",
      "2 [0.04166667 0.875      0.08333333] 1\n",
      "0 [0.04347826 0.47826087 0.47826087] 1\n",
      "0 [0.03846154 0.11538462 0.84615385] 2\n",
      "1 [0.4 0.4 0.2] 0\n",
      "2 [0.02173913 0.91304348 0.06521739] 1\n",
      "2 [0.375 0.375 0.25 ] 0\n",
      "1 [0.5  0.25 0.25] 0\n",
      "2 [0.625 0.25  0.125] 0\n",
      "2 [0.6 0.2 0.2] 0\n",
      "2 [0.25 0.5  0.25] 1\n",
      "2 [0.16666667 0.5        0.33333333] 1\n",
      "2 [0.16666667 0.66666667 0.16666667] 1\n"
     ]
    }
   ],
   "source": [
    "alpha=1\n",
    "beta=0.1\n",
    "iterations=30\n",
    "for i in range(iterations):\n",
    "    for j in range(len(docs_struct)):\n",
    "        for k in range(len(docs_struct[j])):\n",
    "            topic=topic_alloc[j][k]\n",
    "            word=docs_struct[j][k]\n",
    "            \n",
    "            docs_pertopic_count[j][topic]-=1\n",
    "            word_topic_count[topic][word-1]-=1\n",
    "            p=[]\n",
    "            for t in range(n_topics):\n",
    "                indoc_topic_count=docs_pertopic_count[j][t]+alpha\n",
    "                indoc_alltopics_count=sum(docs_pertopic_count[j])+n_topics*alpha\n",
    "            \n",
    "                alldoc_word_topic_count=word_topic_count[t][word-1]+beta\n",
    "                alldoc_word_alltopic_count=np.sum(word_topic_count,axis=0)[word-1]+len(vocab)*beta\n",
    "                p.append((indoc_topic_count/indoc_alltopics_count)*(alldoc_word_topic_count/alldoc_word_alltopic_count))\n",
    "            p=p/sum(p)\n",
    "            new_topic=np.argmax(p)\n",
    "            if topic!=new_topic:\n",
    "                print(topic,p,new_topic)\n",
    "            docs_pertopic_count[j][new_topic]+=1\n",
    "            word_topic_count[new_topic][word-1]+=1\n",
    "            topic_alloc[j][k]=new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 2., 2.],\n",
       "       [2., 1., 0.],\n",
       "       [0., 4., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [5., 1., 0.],\n",
       "       [3., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pertopic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air' 'aspiring' 'cool' 'movie' 'museum' 'snail' 'star' 'turtle']\n",
      "['cake' 'day' 'holiday' 'race' 'space' 'time' 'travel']\n",
      "['thanksgiving' 'turkey']\n"
     ]
    }
   ],
   "source": [
    "topic1=[]\n",
    "topic2=[]\n",
    "topic3=[]\n",
    "for j in range(len(docs_struct)):\n",
    "        for k in range(len(docs_struct[j])):\n",
    "            topic=topic_alloc[j][k]\n",
    "            word=vocab[docs_struct[j][k]-1]\n",
    "            if topic==0:\n",
    "                topic1.append(word)\n",
    "            elif topic==1:\n",
    "                topic2.append(word)\n",
    "            elif topic==2:\n",
    "                topic3.append(word)\n",
    "                \n",
    "print(np.unique(np.array(topic1)))\n",
    "print(np.unique(np.array(topic2)))\n",
    "print(np.unique(np.array(topic3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
