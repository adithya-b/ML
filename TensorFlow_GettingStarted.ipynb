{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x=tf.Variable(3,name=\"x\")\n",
    "y=tf.Variable(4,name=\"y\")\n",
    "f=x*x*y+y+2\n",
    "f2=x*x+y\n",
    "g=tf.global_variables_initializer()\n",
    "\n",
    "f.graph is tf.get_default_graph()# every node is added to default graph\n",
    "graph=tf.Graph()#Creating a new graph and adding the nodes to the same.\n",
    "with graph.as_default():\n",
    "    x2=tf.Variable(5)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 13\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    g.run()\n",
    "    result=f.eval()\n",
    "    f_val,f2_val=sess.run([f,f2])\n",
    "    print(f_val,f2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7421875e+01]\n",
      " [ 4.3584442e-01]\n",
      " [ 9.3460083e-03]\n",
      " [-1.0673523e-01]\n",
      " [ 6.4443970e-01]\n",
      " [-4.2263418e-06]\n",
      " [-3.7744045e-03]\n",
      " [-4.2620850e-01]\n",
      " [-4.4000244e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m, n=housing.data.shape\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data]\n",
    "\n",
    "x=tf.constant(housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "xt=tf.transpose(x)\n",
    "theta=tf.matmul(tf.matrix_inverse(tf.matmul(xt,x)),tf.matmul(xt,y))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value=theta.eval()\n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 mse: 8.659345\n",
      "epoch:  100 mse: 5.035349\n",
      "epoch:  200 mse: 4.922659\n",
      "epoch:  300 mse: 4.8889575\n",
      "epoch:  400 mse: 4.867003\n",
      "epoch:  500 mse: 4.8510027\n",
      "epoch:  600 mse: 4.8391786\n",
      "epoch:  700 mse: 4.830402\n",
      "epoch:  800 mse: 4.8238635\n",
      "epoch:  900 mse: 4.818972\n",
      "[[-0.45982814]\n",
      " [ 0.8637171 ]\n",
      " [ 0.15735266]\n",
      " [-0.269707  ]\n",
      " [ 0.28265893]\n",
      " [ 0.00909606]\n",
      " [-0.04331845]\n",
      " [-0.5864663 ]\n",
      " [-0.5581758 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "scaler=StandardScaler()\n",
    "housing_data_plus_bias_scaled=scaler.fit_transform(housing_data_plus_bias)#scaling important for gradient descent\n",
    "x=tf.constant(housing_data_plus_bias_scaled,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "xt=tf.transpose(x)\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")\n",
    "\n",
    "y_pred=tf.matmul(x,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradients=(2/m)*tf.matmul(xt,error)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100 == 0:\n",
    "            print(\"epoch: \",epoch,\"mse:\",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "A=tf.placeholder(tf.float32,shape=(None,None))\n",
    "f=A+3\n",
    "with tf.Session() as sess:\n",
    "    val=f.eval(feed_dict={A:[[1],[2]]})\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75101876]\n",
      " [ 0.8250937 ]\n",
      " [ 0.11623652]\n",
      " [-0.21509653]\n",
      " [ 0.3208811 ]\n",
      " [-0.00621601]\n",
      " [ 0.00419882]\n",
      " [-0.85742074]\n",
      " [-0.830659  ]]\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mymodel.ckpt\n",
      "4.809901\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run_{}/\".format(root_logdir,now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_epochs=10\n",
    "learning_rate=0.01\n",
    "scaler=StandardScaler()\n",
    "housing_data_plus_bias_scaled=scaler.fit_transform(housing_data_plus_bias)#scaling important for gradient descent\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=(None,n+1),name=\"x\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = housing_data_plus_bias_scaled[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")\n",
    "\n",
    "y_pred=tf.matmul(x,theta,name=\"predictions\")\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error=y_pred-y\n",
    "    mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "#saver\n",
    "saver=tf.train.Saver()\n",
    "#summary\n",
    "mse_summary=tf.summary.scalar(\"MSE\",mse)\n",
    "fileWriter=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            x_batch,y_batch=fetch_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op,feed_dict={x: x_batch,y: y_batch})  \n",
    "            if batch_index%10 == 0:\n",
    "                summary_str=mse_summary.eval(feed_dict={x:housing_data_plus_bias_scaled,y:housing.target.reshape(-1,1)})\n",
    "                step=epoch*n_batches+batch_index\n",
    "                fileWriter.add_summary(summary_str,step)\n",
    "        if epoch%100 == 0:\n",
    "            save_path=saver.save(sess,\"/tmp/mymodel.ckpt\")\n",
    "    best_theta=theta.eval()\n",
    "    save_path=saver.save(sess,\"/tmp/mymodel.ckpt\")\n",
    "    print(best_theta)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"/tmp/mymodel.ckpt\")\n",
    "    print(mse.eval(feed_dict={x:housing_data_plus_bias_scaled,y:housing.target.reshape(-1,1)}))\n",
    "fileWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run_{}/\".format(root_logdir,now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.constant(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b)\n",
    "        return tf.maximum(0.0,z,name=\"relu_result\")\n",
    "    \n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run_{}/\".format(root_logdir,now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        with tf.variable_scope(\"shared\",reuse=True) as scope:\n",
    "            threshold=tf.get_variable(\"threshold\")\n",
    "            dummy=tf.constant(0.0,name=\"dummy\")\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.constant(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b)\n",
    "        return tf.maximum(0.0,z,name=\"relu_result\")\n",
    "    \n",
    "n_features=3\n",
    "with tf.variable_scope(\"shared\") as scope:\n",
    "    threshold=tf.get_variable(\"threshold\",shape=[1],initializer=tf.constant_initializer(0.0))\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
