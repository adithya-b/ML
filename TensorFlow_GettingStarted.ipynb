{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 110 110\n"
     ]
    }
   ],
   "source": [
    "#Variables with global variables intializer and session \n",
    "import tensorflow as tf\n",
    "\n",
    "x=tf.Variable(7,name=\"X\")\n",
    "y=tf.Variable(6,name=\"Y\")\n",
    "z=x*x+y\n",
    "f1=z*2\n",
    "f2=z*2\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    z_val=z.eval()\n",
    "    f1_val=f1.eval()\n",
    "    f2_val=f2.eval()\n",
    "    #Better way of evaluating the above statements requires half computation as the same graph is used in both\n",
    "    f1_val,f2_val=sess.run([f1,f2])\n",
    "    print(z_val,f1_val,f2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using a different graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    x2=tf.Variable(60,name=\"X2\")\n",
    "\n",
    "x2.graph is g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1351172e-01]\n",
      " [ 1.5650615e-02]\n",
      " [-1.8251595e-01]\n",
      " [ 8.6510789e-01]\n",
      " [ 7.7918567e-06]\n",
      " [-4.6992572e-03]\n",
      " [-6.3955665e-02]\n",
      " [-1.6385417e-02]]\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression using formula ((xt.x)^-1).x.y)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "x=tf.constant(housing.data,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"Y\")#reshaping as the actual variable \n",
    "                                                                    #shape comes as (20640,)\n",
    "\n",
    "xt=tf.transpose(x)\n",
    "\n",
    "theta=tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(xt,x)),xt),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_val=theta.eval()\n",
    "    print(theta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5208672e-01]\n",
      " [ 1.3115743e-01]\n",
      " [-2.9265052e-01]\n",
      " [ 3.2138538e-01]\n",
      " [-2.8180477e-04]\n",
      " [-4.0854704e-02]\n",
      " [-7.8835082e-01]\n",
      " [-7.6087976e-01]\n",
      " [-8.8462591e-01]]\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "#Using Gradient Descent with direct differential function and gradients function of tensorflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "scaler=StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "x=tf.constant(housing_data_scaled,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"Y\")#reshaping as the actual variable \n",
    "                                                                    #shape comes as (20640,)\n",
    "    \n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(x,theta)\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradient=2/m * tf.matmul(tf.transpose(x),error)\n",
    "#gradient=tf.gradients(mse,[theta])[0] #Gives the gradient values directly\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradient)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    print(theta.eval())\n",
    "    print(gradient[0].get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84918886]\n",
      " [ 0.13703975]\n",
      " [-0.275159  ]\n",
      " [ 0.30163756]\n",
      " [ 0.00189658]\n",
      " [-0.04129073]\n",
      " [-0.7482023 ]\n",
      " [-0.7198128 ]\n",
      " [-0.85978866]]\n"
     ]
    }
   ],
   "source": [
    "#Using Gradient Descent with optimizers which does the differentiation and updates the weights by itself.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "scaler=StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "x=tf.constant(housing_data_scaled,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"Y\")#reshaping as the actual variable \n",
    "                                                                    #shape comes as (20640,)\n",
    "    \n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(x,theta)\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    print(theta.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "#Usage of Placeholders\n",
    "tf.reset_default_graph()\n",
    "A=tf.placeholder(tf.float32,shape=(1,3),name=\"A\")\n",
    "B=tf.placeholder(tf.float32,shape=(1,3),name=\"B\")\n",
    "C=A+B\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c_val=C.eval(feed_dict={A:[[1,2,3]],B:[[1,2,3]]})#Give only one value in feed dictionary\n",
    "    print(c_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83778447]\n",
      " [ 0.10645497]\n",
      " [-0.25947905]\n",
      " [ 0.29196447]\n",
      " [ 0.00181689]\n",
      " [ 0.2128084 ]\n",
      " [-0.8903469 ]\n",
      " [-0.8524218 ]\n",
      " [ 0.42640734]]\n"
     ]
    }
   ],
   "source": [
    "#Mini batch gradient descent using placeholders and splitting the training data into small sets\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "batch_size=100\n",
    "\n",
    "def feed_next_batch(x,y,batch_size,batch_index,epoch,n_batches):\n",
    "    m,n=x.shape\n",
    "    np.random.seed(epoch*n_batches+batch_index)\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    return x[indices],y[indices]\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "scaler=StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"Y\")\n",
    "\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(x,theta)\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            x_batch,y_batch=feed_next_batch(housing_data_scaled,housing.target.reshape(-1,1),batch_size,batch_index,epoch,n_batches)\n",
    "            sess.run(training_op,feed_dict={x:x_batch,y:y_batch})\n",
    "    print(theta.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83778447]\n",
      " [ 0.10645497]\n",
      " [-0.25947905]\n",
      " [ 0.29196447]\n",
      " [ 0.00181689]\n",
      " [ 0.2128084 ]\n",
      " [-0.8903469 ]\n",
      " [-0.8524218 ]\n",
      " [-0.4506507 ]]\n",
      "INFO:tensorflow:Restoring parameters from /tmp/saved_model.ckpt\n",
      "[[ 0.83778447]\n",
      " [ 0.10645497]\n",
      " [-0.25947905]\n",
      " [ 0.29196447]\n",
      " [ 0.00181689]\n",
      " [ 0.2128084 ]\n",
      " [-0.8903469 ]\n",
      " [-0.8524218 ]\n",
      " [-0.4506507 ]]\n"
     ]
    }
   ],
   "source": [
    "#Saving the model using saver\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "batch_size=100\n",
    "\n",
    "def feed_next_batch(x,y,batch_size,batch_index,epoch,n_batches):\n",
    "    m,n=x.shape\n",
    "    np.random.seed(epoch*n_batches+batch_index)\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    return x[indices],y[indices]\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "scaler=StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"Y\")\n",
    "\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(x,theta)\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()# Saver initialization\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            x_batch,y_batch=feed_next_batch(housing_data_scaled,housing.target.reshape(-1,1),batch_size,batch_index,epoch,n_batches)\n",
    "            sess.run(training_op,feed_dict={x:x_batch,y:y_batch})\n",
    "        if(epoch%100==0):\n",
    "            saver.save(sess,\"/tmp/saved_model.ckpt\")#Saver saving the model\n",
    "    print(theta.eval())\n",
    "    saver.save(sess,\"/tmp/saved_model.ckpt\")#Saver saving the model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"/tmp/saved_model.ckpt\")\n",
    "    print(theta.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83778447]\n",
      " [ 0.10645497]\n",
      " [-0.25947905]\n",
      " [ 0.29196447]\n",
      " [ 0.00181689]\n",
      " [ 0.2128084 ]\n",
      " [-0.8903469 ]\n",
      " [-0.8524218 ]\n",
      " [ 0.64220357]]\n"
     ]
    }
   ],
   "source": [
    "#Tensorboard \n",
    "#Mini batch gradient descent using placeholders and splitting the training data into small sets\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=1000\n",
    "batch_size=100\n",
    "\n",
    "#initializing randomnamed logs directory as it would rewrite previous changes if same directory is given\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir=\"tf_logs/run-{}/\".format(timestamp)\n",
    "\n",
    "def feed_next_batch(x,y,batch_size,batch_index,epoch,n_batches):\n",
    "    m,n=x.shape\n",
    "    np.random.seed(epoch*n_batches+batch_index)\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    return x[indices],y[indices]\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "housing_data_plus_bias=np.c_[housing.data,np.ones((m,1))] #adding bias feature as 1 for all entities\n",
    "\n",
    "scaler=StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"Y\")\n",
    "\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(x,theta)\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "mse_summary=tf.summary.scalar(\"MSE\",mse)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            x_batch,y_batch=feed_next_batch(housing_data_scaled,housing.target.reshape(-1,1),batch_size,batch_index,epoch,n_batches)\n",
    "            sess.run(training_op,feed_dict={x:x_batch,y:y_batch})\n",
    "        if(epoch%100==0):\n",
    "            summary_str=mse_summary.eval(feed_dict={x:housing_data_scaled,y:housing.target.reshape(-1,1)})\n",
    "            step=epoch%100\n",
    "            file_writer.add_summary(summary_str,step)\n",
    "    print(theta.eval())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope_name/var\n"
     ]
    }
   ],
   "source": [
    "#Name Scopes\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"scope_name\") as scope:\n",
    "    var=tf.Variable(5,name=\"var\")\n",
    "print(var.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modularity with Name Scope\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir=\"tf_logs/run-{}/\".format(timestamp)\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        m,n=X.get_shape()\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal((int(n),1)),name=\"weights\")\n",
    "        bias=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),bias,name=\"z\")\n",
    "        relu=tf.maximum(0.0,z,name=\"relu\")\n",
    "        return relu\n",
    "    \n",
    "n_features=3\n",
    "x=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(x) for i in range(3)]\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "output=tf.add_n(relus,name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sharing Variables\n",
    "#Use tf.AUTO_REUSE and this takes care of the reusing of the variable without having to set strictly to True or False\n",
    "tf.reset_default_graph()\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir=\"tf_logs/run-{}/\".format(timestamp)\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        with tf.variable_scope(\"shared\",reuse=tf.AUTO_REUSE) as scope:\n",
    "            threshold=tf.get_variable(\"threshold\")\n",
    "        m,n=X.get_shape()\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal((int(n),1)),name=\"weights\")\n",
    "        bias=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),bias,name=\"z\")\n",
    "        relu=tf.maximum(threshold,z,name=\"relu\")\n",
    "        return relu\n",
    "    \n",
    "n_features=3\n",
    "x=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "with tf.variable_scope(\"shared\",reuse=tf.AUTO_REUSE) as scope:\n",
    "            threshold=tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0))\n",
    "relus=[relu(x) for i in range(3)]\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "output=tf.add_n(relus,name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
