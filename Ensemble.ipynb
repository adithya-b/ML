{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble ##\n",
    "* A group of predictors is called an ensemble and thus the technique is called ensemble learning and an ensemble learning algorithm is called an Ensemble method.\n",
    "\n",
    "** Popular Ensemble Methods **\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking\n",
    "\n",
    "** Voting Classifiers **\n",
    "* Aggregating classifiers and predicting with the most votes i.e. a majority vote classifier is called a hard voting classifier.\n",
    "\n",
    "* Even if some of the classifiers are weak learners(meaning slightly better than random guessing) the ensemble can be a strong learner,given there are sufficient number of weak learners and they are sufficiently diverse.\n",
    "\n",
    " \n",
    "** Use diverse predictors for ensemble as this may improve the ensembles accuracy as this increases the chances that they will make very different types of errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SVC', 0.88800000000000001)\n",
      "('RandomForestClassifier', 0.872)\n",
      "('SGDClassifier', 0.85599999999999998)\n",
      "('VotingClassifier', 0.89600000000000002)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sv_clf=SVC(random_state=42)\n",
    "rnd_clf=RandomForestClassifier(random_state=42)\n",
    "log_clf=LogisticRegression(random_state=42)\n",
    "\n",
    "voting_clf=VotingClassifier(\n",
    "                             estimators=[(\"sv_clf\",sv_clf),\n",
    "                                        (\"rnd_clf\",rnd_clf),\n",
    "                                        (\"log_clf\",log_clf)],\n",
    "                             voting=\"hard\"\n",
    "                            )\n",
    "\n",
    "for clf in [sv_clf,rnd_clf,sgd_clf,voting_clf]:\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Soft Voting **\n",
    "* Instead of going for majority vote if the probabilities are considered and average is taken and classification is done based on same.\n",
    "* For soft voting the estimators must have predict_proba as a function and that is they have to estimate the probabilites.\n",
    "* For above changing the voting to \"soft\" will transform it to soft classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bagging And Pasting**\n",
    "\n",
    "* Using same training algorithm for every predictor but training on different random subsets of the training set.\n",
    "* Sampling done with replacement is called bagging(bootstrap aggregating)\n",
    "* Sampling done without replacement is called Pasting\n",
    "* For aggregation of results typically statistical mode(most frequent) is used for classification and average is used for aggregation.\n",
    "\n",
    "** Scaling Aspect **\n",
    "* The above methods are highly scalable as training and predictions can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90400000000000003"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf=BaggingClassifier(DecisionTreeClassifier(random_state=42),n_estimators=500,max_samples=100,n_jobs=-1,bootstrap=True,random_state=42)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "\n",
    "accuracy_score(y_test,bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note **\n",
    "* BaggingClassifier automatically performs soft voting if the base classifier has predict_proba\n",
    "* For pasting bootstrap needs to be set to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Out of Bag Evaluation **\n",
    "* With bagging(bootstrap= True) during sampling only some of the samples(63%) are used for training of the data and the remaining 37% are called out of bag(oob) instances.\n",
    "* They are not the same 37% for all predictors.\n",
    "* Now a cross validation can be made possible on evaluating the predictors on the oob instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888\n",
      "0.904\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=42),\n",
    "                             n_estimators=500,n_jobs=-1,bootstrap=True,\n",
    "                             oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "print(bag_clf.oob_score_)\n",
    "y_pred=bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42613636,  0.57386364],\n",
       "       [ 0.37096774,  0.62903226],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.09142857,  0.90857143],\n",
       "       [ 0.38118812,  0.61881188],\n",
       "       [ 0.01604278,  0.98395722],\n",
       "       [ 0.9902439 ,  0.0097561 ],\n",
       "       [ 0.96276596,  0.03723404],\n",
       "       [ 0.8       ,  0.2       ],\n",
       "       [ 0.00578035,  0.99421965],\n",
       "       [ 0.78172589,  0.21827411],\n",
       "       [ 0.82065217,  0.17934783],\n",
       "       [ 0.96236559,  0.03763441],\n",
       "       [ 0.07978723,  0.92021277],\n",
       "       [ 0.00571429,  0.99428571],\n",
       "       [ 0.975     ,  0.025     ],\n",
       "       [ 0.92899408,  0.07100592],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02688172,  0.97311828],\n",
       "       [ 0.35078534,  0.64921466],\n",
       "       [ 0.89204545,  0.10795455],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99029126,  0.00970874],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.66831683,  0.33168317],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.15263158,  0.84736842],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.38613861,  0.61386139],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.22159091,  0.77840909],\n",
       "       [ 0.34042553,  0.65957447],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02970297,  0.97029703],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00555556,  0.99444444],\n",
       "       [ 0.97905759,  0.02094241],\n",
       "       [ 0.9137931 ,  0.0862069 ],\n",
       "       [ 0.95854922,  0.04145078],\n",
       "       [ 0.97109827,  0.02890173],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04830918,  0.95169082],\n",
       "       [ 0.98913043,  0.01086957],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.0052356 ,  0.9947644 ],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.77192982,  0.22807018],\n",
       "       [ 0.40555556,  0.59444444],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.68539326,  0.31460674],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.84357542,  0.15642458],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.60621762,  0.39378238],\n",
       "       [ 0.13824885,  0.86175115],\n",
       "       [ 0.66477273,  0.33522727],\n",
       "       [ 0.91176471,  0.08823529],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.19354839,  0.80645161],\n",
       "       [ 0.89142857,  0.10857143],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.06153846,  0.93846154],\n",
       "       [ 0.02645503,  0.97354497],\n",
       "       [ 0.27807487,  0.72192513],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.83823529,  0.16176471],\n",
       "       [ 0.00568182,  0.99431818],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.22      ,  0.78      ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.92822967,  0.07177033],\n",
       "       [ 0.80628272,  0.19371728],\n",
       "       [ 0.00555556,  0.99444444],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.18888889,  0.81111111],\n",
       "       [ 0.6284153 ,  0.3715847 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.0505618 ,  0.9494382 ],\n",
       "       [ 0.5625    ,  0.4375    ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0273224 ,  0.9726776 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.27368421,  0.72631579],\n",
       "       [ 0.51123596,  0.48876404],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02366864,  0.97633136],\n",
       "       [ 0.98979592,  0.01020408],\n",
       "       [ 0.2688172 ,  0.7311828 ],\n",
       "       [ 0.89130435,  0.10869565],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.80104712,  0.19895288],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01092896,  0.98907104],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98351648,  0.01648352],\n",
       "       [ 0.99444444,  0.00555556],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.95108696,  0.04891304],\n",
       "       [ 0.99438202,  0.00561798],\n",
       "       [ 0.01675978,  0.98324022],\n",
       "       [ 0.25301205,  0.74698795],\n",
       "       [ 0.98089172,  0.01910828],\n",
       "       [ 0.28571429,  0.71428571],\n",
       "       [ 0.98550725,  0.01449275],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.66666667,  0.33333333],\n",
       "       [ 0.34104046,  0.65895954],\n",
       "       [ 0.47647059,  0.52352941],\n",
       "       [ 0.84293194,  0.15706806],\n",
       "       [ 0.94535519,  0.05464481],\n",
       "       [ 0.07222222,  0.92777778],\n",
       "       [ 0.80392157,  0.19607843],\n",
       "       [ 0.01612903,  0.98387097],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01041667,  0.98958333],\n",
       "       [ 0.98947368,  0.01052632],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01015228,  0.98984772],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00485437,  0.99514563],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96315789,  0.03684211],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.44886364,  0.55113636],\n",
       "       [ 0.29281768,  0.70718232],\n",
       "       [ 0.00478469,  0.99521531],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.40223464,  0.59776536],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.97524752,  0.02475248],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.61666667,  0.38333333],\n",
       "       [ 0.94329897,  0.05670103],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98837209,  0.01162791],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.09195402,  0.90804598],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.04255319,  0.95744681],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.03092784,  0.96907216],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.94210526,  0.05789474],\n",
       "       [ 0.76097561,  0.23902439],\n",
       "       [ 0.57142857,  0.42857143],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.16374269,  0.83625731],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.95808383,  0.04191617],\n",
       "       [ 0.95897436,  0.04102564],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00515464,  0.99484536],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.41975309,  0.58024691],\n",
       "       [ 0.84408602,  0.15591398],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00628931,  0.99371069],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.95      ,  0.05      ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.2295082 ,  0.7704918 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98412698,  0.01587302],\n",
       "       [ 0.81564246,  0.18435754],\n",
       "       [ 0.99489796,  0.00510204],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 0.07017544,  0.92982456],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02051282,  0.97948718],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05084746,  0.94915254],\n",
       "       [ 0.99459459,  0.00540541],\n",
       "       [ 0.77456647,  0.22543353],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.88770053,  0.11229947],\n",
       "       [ 0.9787234 ,  0.0212766 ],\n",
       "       [ 0.19796954,  0.80203046],\n",
       "       [ 0.24489796,  0.75510204],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00571429,  0.99428571],\n",
       "       [ 0.23529412,  0.76470588],\n",
       "       [ 0.98412698,  0.01587302],\n",
       "       [ 0.00558659,  0.99441341],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.49468085,  0.50531915],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.0802139 ,  0.9197861 ],\n",
       "       [ 0.14594595,  0.85405405],\n",
       "       [ 0.97790055,  0.02209945],\n",
       "       [ 0.01111111,  0.98888889],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.37967914,  0.62032086],\n",
       "       [ 0.12371134,  0.87628866],\n",
       "       [ 0.46315789,  0.53684211],\n",
       "       [ 0.57526882,  0.42473118],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.57142857,  0.42857143],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.26666667,  0.73333333],\n",
       "       [ 0.83707865,  0.16292135],\n",
       "       [ 0.06179775,  0.93820225],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.81151832,  0.18848168],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.1010101 ,  0.8989899 ],\n",
       "       [ 0.01092896,  0.98907104],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99479167,  0.00520833],\n",
       "       [ 0.89325843,  0.10674157],\n",
       "       [ 0.14136126,  0.85863874],\n",
       "       [ 0.91525424,  0.08474576],\n",
       "       [ 0.00568182,  0.99431818],\n",
       "       [ 0.59090909,  0.40909091],\n",
       "       [ 0.05882353,  0.94117647],\n",
       "       [ 0.98837209,  0.01162791],\n",
       "       [ 0.79057592,  0.20942408],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.92972973,  0.07027027],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.26486486,  0.73513514],\n",
       "       [ 0.98941799,  0.01058201],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.89677419,  0.10322581],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.75      ,  0.25      ],\n",
       "       [ 0.94652406,  0.05347594],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.67368421,  0.32631579],\n",
       "       [ 0.54705882,  0.45294118],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.92857143,  0.07142857],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.85643564,  0.14356436],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.71823204,  0.28176796],\n",
       "       [ 0.11413043,  0.88586957],\n",
       "       [ 0.51412429,  0.48587571],\n",
       "       [ 0.25136612,  0.74863388],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.91176471,  0.08823529],\n",
       "       [ 0.77325581,  0.22674419],\n",
       "       [ 0.01123596,  0.98876404],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98870056,  0.01129944],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01612903,  0.98387097],\n",
       "       [ 0.96195652,  0.03804348],\n",
       "       [ 0.95408163,  0.04591837],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.58928571,  0.41071429],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99484536,  0.00515464],\n",
       "       [ 0.01714286,  0.98285714],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98787879,  0.01212121],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04830918,  0.95169082],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0273224 ,  0.9726776 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.13917526,  0.86082474],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01098901,  0.98901099],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.39204545,  0.60795455],\n",
       "       [ 0.04891304,  0.95108696],\n",
       "       [ 0.25149701,  0.74850299],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98395722,  0.01604278],\n",
       "       [ 0.21578947,  0.78421053],\n",
       "       [ 0.99484536,  0.00515464],\n",
       "       [ 0.00531915,  0.99468085],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.95505618,  0.04494382],\n",
       "       [ 0.31351351,  0.68648649],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98901099,  0.01098901],\n",
       "       [ 0.0106383 ,  0.9893617 ],\n",
       "       [ 0.03333333,  0.96666667],\n",
       "       [ 0.97916667,  0.02083333],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.03763441,  0.96236559],\n",
       "       [ 0.69714286,  0.30285714]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Patches N Random Subspaces**\n",
    "* As max_samples and bootstrap is for training set sampling max_features and bootstrap_features is for featire sampling\n",
    "* Sampling both training set and feature set is called Random Patches\n",
    "* Sampling feature set but keeping all training set(bootstrap=false and max_sample=1.0) is called Random subspaces method.\n",
    "* Sampling features results in bit more diversity trading bit more bias for lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forests **\n",
    "* Random forests are ensemble of decisiontreeclassifiers generally trained via Bagging(sometimes pasting) typically with max_samples set to size of the training set.\n",
    "* RF is optimized Bagging of dcf\n",
    "* with few exceptions RF has all parameters of DCF and Bagging\n",
    "* It also brings randomness in selection of best feature when splitting node by selecting the same from random subset of features.Again trading more bias for lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "rf_clf=RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1,random_state=42)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred_rf=rnd_clf.predict(X_test)\n",
    "\n",
    "#Bagging rough equivalent of RF\n",
    "bag_clf=BaggingClassifier(DecisionTreeClassifier(splitter=\"random\",max_leaf_nodes=16)\n",
    "                          ,n_estimators=500,max_samples=1.0,bootstrap=True,n_jobs=-1,random_state=42)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred_bag=bag_clf.predict(X_test)\n",
    "print(np.float(np.sum(y_pred_rf == y_pred_bag))/len(y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extra Trees **\n",
    "* Random forest can be made more random by selecting feature more randomly by using random thresholds.\n",
    "* These are called Extremely Randomized Trees\n",
    "* sklearn: ExtraTreesClassifier/ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Feature Importances **\n",
    "* Feature importances in Random Forest can be calculated by averaging the depth at which the feature appeared across all trees .\n",
    "* Can be accessed through feature_importances_ variable.\n",
    "* Can be used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Boosting **\n",
    "* Refers to any ensemble method that can combine several weak learners into a strong learner\n",
    "* Main idea is to train sequentially,each trying to correct it's predecessor.\n",
    "* Most popular one's:\n",
    "* Adaptive Boosting(Adaboost)\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** AdaBoost **\n",
    "\n",
    "* Adaboost involves assigning weights to different training instances and construction of successor predictor such that more weight is assigned to underfitted instances in the predecessor.\n",
    "\n",
    "* Each predictor is given a weight based on their performance and the same weight is used in assigning weight to the misclassified instances in successor predictor\n",
    "\n",
    "* The prediction is made based on the a value calculated by weighted averaging of the probabilities or taking the weight of the predictors which classified the instance as one particular class.\n",
    "\n",
    "* Sklearn uses multiclass version of AdaBoost called Stagewise Additive Modelling using a MultiClass Exponential loss function.\n",
    "\n",
    "* Sklearn also uses a variant of SAMME called SAMME.R(R stands for Real) which relies on class probabilities rather than predictions.\n",
    "\n",
    "** Drawback **\n",
    "* The main drawback is it cannot be parallelized(only partially) since each predcitor is only trained after the predecessor training as it is dependent on the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,algorithm=\"SAMME.R\",learning_rate=0.5)\n",
    "ada_clf.fit(X_train,y_train)\n",
    "print(accuracy_score(y_test,ada_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
