{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(raw_text,language=\"english\"):\n",
    "    default_st=nltk.sent_tokenize\n",
    "    default_wt=nltk.word_tokenize\n",
    "    sentences=default_st(raw_text,language=language)\n",
    "    words=[default_wt(sentence) for sentence in sentences]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern=re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution=r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word=repeat_pattern.sub(match_substitution,old_word)\n",
    "        if old_word!=new_word:\n",
    "            return replace(new_word)\n",
    "        else:\n",
    "            return new_word\n",
    "    corrected_tokens=[replace(token) for token in tokens]\n",
    "    return corrected_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am', 'really', 'awesome']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence=\"aaamm realllyy aweeesoomme\"\n",
    "tokens=tokenize_text(sample_sentence)[0]\n",
    "corrected_tokens=remove_repeated_characters(tokens)\n",
    "corrected_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import suggest\n",
    "print(\"finallly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stemming **\n",
    "* Finding the stem of a word.\n",
    "* There are many stemmer library modules available.They may behave differently for few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "ps.stem(\"lying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lying'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls=LancasterStemmer()\n",
    "ls.stem(\"lying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer#Used for stemming of other languages\n",
    "ss=SnowballStemmer(\"german\")\n",
    "print(ss.stem(\"lying\"))\n",
    "ss.stem(\"autobahnen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "* Same as Stemming but finds out the root word instead of the root stem.\n",
    "* The root word changes based on the POS given as the parameter.\n",
    "* WordNetLemmatizer internally uses the morphy() function belonging to the WordNetCorpusReader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "wnl.lemmatize(\"cars\",\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize(\"running\",\"v\"))\n",
    "print(wnl.lemmatize(\"fancier\",\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** POS Tagger Library Versions**\n",
    "* Used for obtaining the POS tags given tokens as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('brown', 'ADJ'),\n",
       " ('fox', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('quick', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('he', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('jumping', 'VERB'),\n",
       " ('over', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('lazy', 'ADJ'),\n",
       " ('dog', 'NOUN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence=\"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "tagged_sent=nltk.pos_tag(tokens,tagset=\"universal\")\n",
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix installation of pattern\n",
    "from pattern.en import tag\n",
    "tagged_sent=tag(sentence)\n",
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "data=treebank.tagged_sents()\n",
    "train_data=data[:3500]\n",
    "test_data=data[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Custom POS Taggers **\n",
    "* Every tagger is a child class of TaggerIclass and all of them have tag() and evaluate() function\n",
    "* DefaultTagger with base class as SequentialBackoffTagger assings same POS to all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1454158195372253"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "dt=DefaultTagger(\"NN\")\n",
    "dt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'NN'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('is', 'NN'),\n",
       " ('quick', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('he', 'NN'),\n",
       " ('is', 'NN'),\n",
       " ('jumping', 'NN'),\n",
       " ('over', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('lazy', 'NN'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also use RegexpTagger Class with passing it a pair of pattern of words and their POS tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Taggers By NLTK **\n",
    "* Ngrams use Bayesian rule and maximize likelihood to find the POS tags for each sentence.\n",
    "* NLTK provides the library implementations of below implemented Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut=UnigramTagger(train_data)\n",
    "bt=BigramTagger(train_data)\n",
    "tt=TrigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram evaluation: 0.8607803272340013\n",
      "Bigram evaluation: 0.13466937748087907\n",
      "Trigram evaluation: 0.08064672281924679\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram evaluation:\",ut.evaluate(test_data))\n",
    "print(\"Bigram evaluation:\",bt.evaluate(test_data))\n",
    "print(\"Trigram evaluation:\",tt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', None), ('dog', None)]\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(ut.tag(tokens))\n",
    "print(bt.tag(tokens))\n",
    "print(tt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Combination of Ngrams**\n",
    "* As you can observe there are places where the POS tag is given as None. \n",
    "* This is because of no presence of such combination in training data\n",
    "* For Unigram it is the absence of such words in the training data\n",
    "* For Bigram and Trigram it is due to the absence of sequences as they are in Train data.\n",
    "* This can be handled by smoothing which is nothing but the combination of all the above models with few parameters so that if there are combinations not present they can fall back to different POS Tagger. For example Trigram->Bigram->Unigram->RegexTagger\n",
    "* The methods used are Linear Interpolation and Discounting Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ClassifierBasedPOSTagger **\n",
    "* Can be used for POS Tag classification.\n",
    "* Provides feature_detector parameter to construct custom function for feature detection\n",
    "* Provides classifier_builder parameter which accept NaiveBayesClassifer MaxentClassifer as input(provided by nltk library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9306806079969019"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt=ClassifierBasedPOSTagger(train=train_data,classifier_builder=NaiveBayesClassifier.train)\n",
    "nbt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('fox', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('quick', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('he', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('jumping', 'VBG'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'VBG')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbt.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
